{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd830a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rmsprop  :  \n",
    "    def __init__(self, beta = .9  ,lr = .001  ,epsilon= 1e-8) : \n",
    "        self.lr = lr \n",
    "        self.beta1 = beta1\n",
    "        self.epsilon = epsilon\n",
    "    def build(self , objects , l ) : \n",
    "        for i in range( 1 , l+1 ): \n",
    "            vars(self)[f'sdW_{i}'] = np.zeros((objects[f'W_{i}'].shape))\n",
    "            vars(self)[f'sdb_{i}'] = np.zeros((objects[f'b_{i}'].shape))\n",
    "    def back_prop(self , dAL, objects , l , lambd , m  , activation ) : \n",
    "         '''\n",
    "            objects : it,s contain all weights and baises and activatoins from the original model \n",
    "            dAl : the gradients of the loss with respect to weights \n",
    "            l : number of layers \n",
    "            lambd : regularization prameter \n",
    "            m : training examples \n",
    "            activation : the activation function \n",
    "        '''\n",
    "        # call method build \n",
    "        self.build(objects , l )\n",
    "        \n",
    "        vars(self)[f'dZ_{l}'] = dAL * sigmoid.backword(objects[f'A_{l}']) \n",
    "        vars(self)[f'dW_{l}'] =1. /m * np.dot(  vars(self)[f'dZ_{l}'] , objects[f'A_{l-1}'].T) + (lambd / m ) * objects[f'W_{l}'] \n",
    "        vars(self)[f'db_{l}'] = np.sum(vars(self)[f'dZ_{l}'] , axis = 1, keepdims =True) / m\n",
    "        vars(self)[f'sdW_{l}'] = self.beta2 * vars(self)[f'sdW_{l}'] + (1-self.beta2) * (vars(self)[f'dW_{l}'] ** 2)\n",
    "        vars(self)[f'sdb_{l}'] = self.beta2 * vars(self)[f'sdb_{l}'] + (1-self.beta2) * (vars(self)[f'db_{l}'] ** 2)\n",
    "        vars(self)[f'dA_{l-1}'] = np.dot(objects[f'W_{l}'].T ,vars(self)[f'dZ_{l}'] ) \n",
    "        for i in reversed(range(1 , l)) : # iteraite backword throw the hidden layers \n",
    "        # compute the gradients for weights and biases for the hidden layers  \n",
    "            vars(self)[f'dZ_{i}'] = vars(self)[f'dA_{i}'] * activation.backword(objects[f'A_{i}'])\n",
    "            vars(self)[f'dW_{i}'] =1. / m *  np.dot( vars(self)[f'dZ_{i}'] ,objects[f'A_{i-1}'].T) + (lambd / m ) * objects[f'W_{i}']\n",
    "            vars(self)[f'db_{i}'] = np.sum(vars(self)[f'dZ_{i}'] , keepdims= True , axis =1) /m\n",
    "            vars(self)[f'sdW_{i}'] = self.beta2 * vars(self)[f'sdW_{i}'] + (1-self.beta2) * (vars(self)[f'dW_{i}'] ** 2)\n",
    "            vars(self)[f'sdb_{i}'] = self.beta2 * vars(self)[f'sdb_{i}'] + (1-self.beta2) * (vars(self)[f'db_{i}'] ** 2)\n",
    "            vars(self)[f'dA_{i-1}'] = np.dot(objects[f'W_{i}'].T ,vars(self)[f'dZ_{i}'] )\n",
    "      \n",
    "      # update the weights and biases\n",
    "        for i in range(1, l+1) : \n",
    "            objects[f'W_{i}'] -= self.lr * (objects[f'dW_{i}']/ (np.sqrt(vars(self)[f'sdW_{i}'] + self.epsilon)))\n",
    "            objects[f'b_{i}'] -= self.lr * (objects[f'db_{i}']/ (np.sqrt(vars(self)[f'sdb_{i}'] + self.epsilon)))\n",
    "   \n",
    "            \n",
    "   \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
